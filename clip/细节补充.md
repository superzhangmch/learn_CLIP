### CLIP用了很大的batchsize=32768, loss要求两两相乘。怎么实现的？
- https://github.com/openai/CLIP/issues/132
- https://github.com/openai/CLIP/issues/29
看以上jongwook的回复，他是作者之一。
摘录如下：
```
Say there are N GPUs and each GPU gets M input pairs. When calculating similarities (and thus the
logit scores across text inputs for each image) a GPU only needs to hold M image features, which
needs to be matmul'ed with NM text features, resulting in an M×NM matrix. This computation is
distributed (i.e. sharded) across N GPUs, and overall we have calculated NM×NM similarities across
the GPUs. The loss we use is symmetric and the same happens w.r.t. text inputs.
```
```
# image_features.shape = [local_batch_size, embed_dim]
# text_features.shape = [local_batch_size, embed_dim]

all_image_features = all_gather(image_features)  # shape = [global_batch_size, embed_dim]
all_text_features = all_gather(text_features)    # shape = [global_batch_size, embed_dim]

logits_per_image = logit_scale * image_features @ all_text_features.t()  # shape = [local_batch_size, global_batch_size]
logits_per_text = logit_scale * text_features @ all_image_features.t()   # shape = [local_batch_size, global_batch_size]
```
即每个GPU需要拿到别的全局的img 与 text features。

### 为什么用32768这么大的batch size？为什么不每一个pair做一个二分类，而是来一个大大的softmax？
关于后一问，看 https://github.com/openai/CLIP/issues/25 里 Newmu 的回答。大概说若用遍历所有 pair的二分类，令model收敛是个麻烦的问题。
另外根据看一些相关资料后看，作者这样做，本身就是一个很标准的对比学习的操作。而为什么用这么大的batch size？（按一些理论，batchsize取太大并不好，容易让模型陷入一些次优解），除了本身数据集太大，必须要大的batchsize才能训完外，另一个原因是对比学习本身就需要很大的batchsize。这点是很多资料提到的。
比如《DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP Training》中提到的：
```
For any positive image-text pair, as there are normally unlimited number (up to the total number of images or texts in
a data set) of negative image-text pairs, it is crucial to include a sufficiently large number of negative pairs in a
contrastive loss to make the representation learning effective, as validated in all related works such as CLIP [27],
Florence [47], OpenCLIP [16], and BASIC [26]. Specifically, BASIC shows that larger batch size, plus larger data set and
larger model, theoretically lead to a better generalization performance.
```
